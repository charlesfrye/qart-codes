## A log for every decision made building evals on this project

Evals need:
- Full Reproducibility
- A form of measurement, ideally automated, that represents ground truth for the task


### Reproducibility
Day 1:
Exploring Weights and Biases. It has the ability to log code at the exact point of the run, even between commits, so we can run evals against changing codebases and know how to return to the highest performing version.

### Measurement
Day 2:

Define "ground truth" - thankfully for us, there's a clear pass-fail criteria of if it's scanned by phones. I use iphone 12 pro to scan.

A great way to score our model/prompt is to generate n images on the same QR code input + prompt, and then calculate the pass@1, pass@2, pass@10, pass@100 for that code + prompt combo. Do for many combos, and report the average pass@n. 

I set up an eval script in eval.py that looks at a directory of images and reports if they're valid QRs or not. It initially used openCV as our eval function.

First big challenge: many codes generated by the current model scan on the iphone, but not on the openCV.

Day 3: Seeking alternatives to OpenCV that better represent Iphone scan quality.
To make this search objective, I ran a script that generates 100+ images on random prompts and codes, and then I manually scanned them with my iphone and sorted them into valid/invalid buckets. We'll use these buckets as ground truth to evaluate our evaluation method! From the set, the iphone can scan 85.3% of the images.

As of 2:49pm oct 31 I'm working on pitting openCV against other methods for how closely they follow the iphone results, and will report back.

and idea I had, while doing the act of manually scanning, is that motion / angles were often helpful to get the iphone to detect the QR code, so if I can't get to good detection immediately with openCV or alternative libraries/methods, the next thing to try is to use image augmentation to warp the images in ways that maybe help the library detect it better (skewing, bluring, rotating, etc.) and accept it if any of the augmentations pass.

Day 4:
Nov 1, I tested three methods: openCV, pyzbar.
Scores:
```
Detector:        OpenCV
True Positives:  44
False Positives: 1
True Negatives:  18
False Negatives: 66

Detector:        Pyzbar
True Positives:  30
False Positives: 1
True Negatives:  18
False Negatives: 80
```
Flops!

And then I tried QReader, which uses YOLO to detect QR codes.
```
Detector:        QReader
True Positives:  110
False Positives: 19
True Negatives:  0
False Negatives: 0
```
oh shit, it ranked the models as all-positive! Which, they should be. Let's check decoded text to gut check:
```
Decoded QR code: ('https://www.instagram.com',)
Decoded QR code: ('https://www.deviantart.com',)
Decoded QR code: ('https://www.deviantart.com',)
Decoded QR code: ('https://www.twitter.com',)
Decoded QR code: ('https://www.vimeo.com',)
Decoded QR code: ('https://www.discord.com',)
```
All are urls from our detector generation.

This surfaces two problems:
1. When we generated the original qr codes, we didn't save which random url they were from. We should have. But the chance of a url mis-read is tiny, so we can choose to ignore this.
2. The OpenCV and Pyzbar methods are worse than the iphone, but the QReader is FAR better than iphone, so would be a bad choice for eval because we want to maximize likilhood our model generates iphone scannable QR codes, so it wouldn't be helpful to thumbs-up 100% of our generations with QReader when iphone can only scan 85.3% of them. 

Reestablishing our goals, we want to maximize likilhood our model generates iphone scannable QR codes. We need evals that detect generated QRs in a way that's representative of iphone scans. 
- If the eval says the generation is 100% perfect (Qreader), the model wouldn't improve. 
- If the eval says the generation is usually quite flawed (openCV, pyzbar), that gives us a labeled set of scannable QRs that we can use for future retraining. But, since openCV and pyzbar likely require "simple" qr images, pushing our model in that direction would make it output more vanilla, boring QRs.

In fact, something we can optionally do here is use openCV or pyzbar as a proxy to measure "boringness" of a QR.