## A log for every decision made building evals on this project

Evals need:
- Full Reproducibility
- A form of measurement, ideally automated, that represents ground truth for the task


### Reproducibility
Day 1:
Exploring Weights and Biases. It has the ability to log code at the exact point of the run, even between commits, so we can run evals against changing codebases and know how to return to the highest performing version.

### Measurement
Day 2:

Define "ground truth" - thankfully for us, there's a clear pass-fail criteria of if it's scanned by phones. I use iphone 12 pro to scan.

A great way to score our model/prompt is to generate n images on the same QR code input + prompt, and then calculate the pass@1, pass@2, pass@10, pass@100 for that code + prompt combo. Do for many combos, and report the average pass@n. 

I set up an eval script in eval.py that looks at a directory of images and reports if they're valid QRs or not. It initially used openCV as our eval function.

First big challenge: many codes generated by the current model scan on the iphone, but not on the openCV.

Day 3: Seeking alternatives to OpenCV that better represent Iphone scan quality.
To make this search objective, I ran a script that generates 100+ images on random prompts and codes, and then I manually scanned them with my iphone and sorted them into valid/invalid buckets. We'll use these buckets as ground truth to evaluate our evaluation method! 

As of 2:49pm oct 31 I'm working on pitting openCV against other methods for how closely they follow the iphone results, and will report back.

and idea I had, while doing the act of manually scanning, is that motion / angles were often helpful to get the iphone to detect the QR code, so if I can't get to good detection immediately with openCV or alternative libraries/methods, the next thing to try is to use image augmentation to warp the images in ways that maybe help the library detect it better (skewing, bluring, rotating, etc.) and accept it if any of the augmentations pass.